# MAEPLE Refactoring Implementation Plan

**Created:** January 4, 2026  
**Status:** âœ… COMPLETE - All Phases Implemented (v2.2.1)  
**Compilation:** âœ… Zero Errors  
**Build Time:** 8.88s  
**Ready for:** Testing

---

## Completion Summary

| Phase | Description | Status |
|-------|-------------|--------|
| Phase 1 | Camera Stability Fix | âœ… Complete (v2.2.1 Enhanced) |
| Phase 2 | Vision Service Enhancement | âœ… Complete |
| Phase 3 | Unified Data Flow | âœ… Complete |
| Phase 4 | Enhanced Logging | âœ… Complete |
| Phase 5 | Correlation Engine | âœ… Complete |

### Files Created
- `src/hooks/useCameraCapture.ts` (317 lines - enhanced v2.2.1)
- `src/contexts/ObservationContext.tsx` (240 lines)
- `src/services/draftService.ts` (388 lines)
- `src/services/correlationService.ts` (397 lines)

### Files Modified
- `src/components/BiofeedbackCameraModal.tsx` (v2.2.1: inline styles, stable dimensions)
- `src/components/StateCheckCamera.tsx`
- `src/components/StateCheckWizard.tsx` (v2.2.1: conditional modal rendering)
- `src/services/geminiVisionService.ts`

### v2.2.1 Camera Fix (January 4, 2026)
- Fixed dependency cascade causing flickering
- Config values (`resolutions`, `maxRetries`) stored in `useRef`
- `initializeCamera` callback has empty dependency array
- `initializeCamera` accepts `facingMode` as explicit parameter
- Main `useEffect` depends ONLY on `isActive` and `facingMode`
- Modal conditionally rendered only when `isOpen` is true
- Video element uses inline `style` for stable dimensions
- Video container has `minHeight: 60vh`

---

## Executive Summary

This document provides a step-by-step implementation guide to fix critical issues in MAEPLE and enhance the core functionality. The plan is organized into phases that can be executed sequentially, with each phase building on the previous one.

**Primary Issues to Resolve:**
1. Camera flickering/instability in Bio-Mirror
2. Analysis chain breaking after photo capture
3. Disconnected data flow between Bio-Mirror and Journal
4. Limited self-reported data logging capabilities

---

## Phase 1: Camera Stability Fix (CRITICAL)

### Problem Analysis

The camera flickering is caused by React re-render cycles triggering camera restarts. Root causes:

1. **`stopCamera` dependency on `stream` state** - Creates new function reference on every stream change
2. **`startCamera` dependency on `stopCamera`** - Cascades the instability
3. **Multiple `useEffect` hooks** - Two separate effects manage camera lifecycle, causing conflicts
4. **`facingMode` state changes** - Triggers camera restart without proper cleanup timing

### Files to Modify

- `src/components/BiofeedbackCameraModal.tsx` (Primary)
- `src/components/StateCheckCamera.tsx` (Secondary - similar issues)

### Implementation Steps

#### Step 1.1: Create Custom Camera Hook

Create a new file: `src/hooks/useCameraCapture.ts`

```typescript
/**
 * useCameraCapture Hook
 * 
 * Provides stable camera management with proper lifecycle handling.
 * Uses refs instead of state for MediaStream to prevent re-renders.
 */

import { useCallback, useEffect, useRef, useState } from 'react';

interface CameraConfig {
  facingMode: 'user' | 'environment';
  maxRetries?: number;
  resolutionPresets?: Array<{ label: string; width: number; height: number }>;
}

interface CameraState {
  isReady: boolean;
  isInitializing: boolean;
  error: string | null;
  currentResolution: string;
}

interface UseCameraCaptureReturn {
  videoRef: React.RefObject<HTMLVideoElement>;
  canvasRef: React.RefObject<HTMLCanvasElement>;
  state: CameraState;
  capture: () => Promise<string | null>;
  switchCamera: () => void;
  retry: () => void;
  facingMode: 'user' | 'environment';
}

const DEFAULT_RESOLUTIONS = [
  { label: 'HD', width: 1280, height: 720 },
  { label: 'SD', width: 720, height: 480 },
  { label: 'Low', width: 480, height: 360 },
];

export function useCameraCapture(
  isActive: boolean,
  config: CameraConfig = { facingMode: 'user' }
): UseCameraCaptureReturn {
  // Use refs for values that shouldn't trigger re-renders
  const streamRef = useRef<MediaStream | null>(null);
  const videoRef = useRef<HTMLVideoElement>(null);
  const canvasRef = useRef<HTMLCanvasElement>(null);
  const mountedRef = useRef(true);
  const retryCountRef = useRef(0);
  
  // State for UI updates only
  const [state, setState] = useState<CameraState>({
    isReady: false,
    isInitializing: false,
    error: null,
    currentResolution: 'HD',
  });
  
  const [facingMode, setFacingMode] = useState(config.facingMode);
  
  const resolutions = config.resolutionPresets || DEFAULT_RESOLUTIONS;
  const maxRetries = config.maxRetries || 3;

  // Stable cleanup function - no dependencies
  const cleanup = useCallback(() => {
    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => {
        try {
          track.stop();
        } catch (e) {
          console.warn('[Camera] Error stopping track:', e);
        }
      });
      streamRef.current = null;
    }
    if (videoRef.current) {
      videoRef.current.srcObject = null;
    }
  }, []);

  // Initialize camera - called by effect
  const initializeCamera = useCallback(async (resolutionIndex: number = 0) => {
    if (!mountedRef.current) return;
    
    // Always cleanup first
    cleanup();
    
    setState(prev => ({ ...prev, isInitializing: true, error: null }));
    
    try {
      const resolution = resolutions[Math.min(resolutionIndex, resolutions.length - 1)];
      
      const constraints: MediaStreamConstraints = {
        video: {
          facingMode: facingMode,
          width: { ideal: resolution.width },
          height: { ideal: resolution.height },
        },
        audio: false,
      };
      
      console.log(`[Camera] Starting at ${resolution.label} resolution...`);
      
      const mediaStream = await navigator.mediaDevices.getUserMedia(constraints);
      
      // Check if still mounted after async operation
      if (!mountedRef.current) {
        mediaStream.getTracks().forEach(track => track.stop());
        return;
      }
      
      streamRef.current = mediaStream;
      retryCountRef.current = 0;
      
      if (videoRef.current) {
        videoRef.current.srcObject = mediaStream;
        
        // Wait for video to be ready
        await new Promise<void>((resolve, reject) => {
          const video = videoRef.current!;
          const timeoutId = setTimeout(() => reject(new Error('Video ready timeout')), 5000);
          
          const handleReady = () => {
            if (video.readyState >= 2) {
              clearTimeout(timeoutId);
              video.removeEventListener('loadedmetadata', handleReady);
              resolve();
            }
          };
          
          video.addEventListener('loadedmetadata', handleReady);
          if (video.readyState >= 2) {
            clearTimeout(timeoutId);
            video.removeEventListener('loadedmetadata', handleReady);
            resolve();
          }
        });
        
        console.log(`[Camera] Ready: ${videoRef.current.videoWidth}x${videoRef.current.videoHeight}`);
      }
      
      if (mountedRef.current) {
        setState({
          isReady: true,
          isInitializing: false,
          error: null,
          currentResolution: resolution.label,
        });
      }
      
    } catch (err) {
      console.error('[Camera] Error:', err);
      
      if (!mountedRef.current) return;
      
      // Try lower resolution on certain errors
      if (err instanceof DOMException) {
        if ((err.name === 'NotReadableError' || err.name === 'OverconstrainedError') 
            && resolutionIndex < resolutions.length - 1) {
          console.warn(`[Camera] Trying lower resolution...`);
          await initializeCamera(resolutionIndex + 1);
          return;
        }
        
        // Map errors to user-friendly messages
        const errorMessages: Record<string, string> = {
          NotAllowedError: 'Camera permission denied. Please enable camera access in your browser settings.',
          NotFoundError: 'No camera found. Please ensure your device has a working camera.',
          NotReadableError: 'Camera is in use by another application. Please close other apps using the camera.',
          OverconstrainedError: 'Camera does not support the requested resolution.',
        };
        
        setState(prev => ({
          ...prev,
          isInitializing: false,
          error: errorMessages[err.name] || `Camera error: ${err.message}`,
        }));
      } else {
        setState(prev => ({
          ...prev,
          isInitializing: false,
          error: 'Unable to access camera. Please try again.',
        }));
      }
    }
  }, [facingMode, cleanup, resolutions]);

  // Capture image
  const capture = useCallback(async (): Promise<string | null> => {
    if (!videoRef.current || !canvasRef.current || !state.isReady) {
      console.error('[Camera] Not ready for capture');
      return null;
    }
    
    const video = videoRef.current;
    const canvas = canvasRef.current;
    
    if (video.videoWidth === 0 || video.videoHeight === 0) {
      console.error('[Camera] Invalid video dimensions');
      return null;
    }
    
    canvas.width = video.videoWidth;
    canvas.height = video.videoHeight;
    
    const ctx = canvas.getContext('2d');
    if (!ctx) return null;
    
    // Mirror for front camera
    if (facingMode === 'user') {
      ctx.translate(canvas.width, 0);
      ctx.scale(-1, 1);
    }
    
    ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
    
    // Return as PNG data URL
    return canvas.toDataURL('image/png');
  }, [state.isReady, facingMode]);

  // Switch camera
  const switchCamera = useCallback(() => {
    setFacingMode(prev => prev === 'user' ? 'environment' : 'user');
  }, []);

  // Retry initialization
  const retry = useCallback(() => {
    if (retryCountRef.current < maxRetries) {
      retryCountRef.current++;
      initializeCamera(0);
    } else {
      setState(prev => ({
        ...prev,
        error: 'Maximum retry attempts reached. Please refresh the page.',
      }));
    }
  }, [initializeCamera, maxRetries]);

  // Main lifecycle effect - single effect for camera management
  useEffect(() => {
    mountedRef.current = true;
    
    if (isActive) {
      initializeCamera(0);
    }
    
    return () => {
      mountedRef.current = false;
      cleanup();
    };
  }, [isActive, facingMode]); // Only re-run when isActive or facingMode changes

  return {
    videoRef,
    canvasRef,
    state,
    capture,
    switchCamera,
    retry,
    facingMode,
  };
}
```

#### Step 1.2: Update BiofeedbackCameraModal.tsx

Replace the camera management logic with the new hook. Key changes:

1. Remove all useState for stream, isVideoReady, isInitializing
2. Remove stopCamera and startCamera callbacks
3. Remove the two separate useEffect hooks for camera
4. Use the new `useCameraCapture` hook
5. Simplify the capture flow

**File:** `src/components/BiofeedbackCameraModal.tsx`

**Changes to make:**

```typescript
// BEFORE (lines 14-35 approximately):
const [stream, setStream] = useState<MediaStream | null>(null);
const [error, setError] = useState<string | null>(null);
const [isCapturing, setIsCapturing] = useState(false);
// ... many more state variables

// AFTER:
import { useCameraCapture } from '../hooks/useCameraCapture';

// Inside component:
const {
  videoRef,
  canvasRef,
  state: cameraState,
  capture: captureImage,
  switchCamera,
  retry: retryCamera,
  facingMode,
} = useCameraCapture(isOpen);

const [isCompressing, setIsCompressing] = useState(false);
const [showCaptureSuccess, setShowCaptureSuccess] = useState(false);
const [showFlash, setShowFlash] = useState(false);
```

**Remove these sections entirely:**
- Lines 67-80: `stopCamera` callback
- Lines 83-162: `startCamera` callback  
- Lines 165-175: `retryCamera` callback
- Lines 178-182: `toggleCamera` callback
- Lines 185-296: `capture` callback (replace with simplified version)
- Lines 347-365: Both useEffect hooks for camera lifecycle

**Add simplified capture handler:**

```typescript
const handleCapture = useCallback(async () => {
  if (!cameraState.isReady) return;
  
  setShowFlash(true);
  setTimeout(() => setShowFlash(false), 100);
  
  if ('vibrate' in navigator) {
    navigator.vibrate(50);
  }
  
  try {
    setIsCompressing(true);
    
    const originalImage = await captureImage();
    if (!originalImage) {
      throw new Error('Failed to capture image');
    }
    
    // Compress the image
    const { compressImage } = await import('../utils/imageCompression');
    const compressedImage = await compressImage(originalImage, {
      maxWidth: 512,
      maxHeight: 512,
      quality: 0.85,
      format: 'image/webp',
    });
    
    setShowCaptureSuccess(true);
    
    setTimeout(() => {
      onCapture(compressedImage);
    }, 800);
    
  } catch (err) {
    console.error('[BiofeedbackCamera] Capture error:', err);
    // Error will be shown via cameraState.error
  } finally {
    setIsCompressing(false);
  }
}, [cameraState.isReady, captureImage, onCapture]);
```

#### Step 1.3: Update StateCheckCamera.tsx

Apply the same pattern as BiofeedbackCameraModal. This component has identical issues.

#### Step 1.4: Add Index Export for Hook

Update `src/hooks/index.ts` (create if doesn't exist):

```typescript
export { useCameraCapture } from './useCameraCapture';
export { usePWAInstall } from './usePWAInstall';
// ... other hooks
```

### Testing Checklist for Phase 1

- [ ] Camera opens without flickering
- [ ] Switching between front/back camera works smoothly
- [ ] Camera closes properly when modal is closed
- [ ] No memory leaks (check DevTools Performance)
- [ ] Error messages display correctly for permission denied
- [ ] Retry functionality works
- [ ] Photo capture produces valid image
- [ ] Image compression completes successfully

---

## Phase 2: Stabilize Bio-Mirror Analysis Chain

### Problem Analysis

After camera capture, the analysis often fails silently or hangs. Issues:
1. Progress bar is simulated, not connected to real progress
2. 30-second timeout may not be enough
3. Circuit breaker state not clearly communicated to user
4. No graceful degradation when AI unavailable

### Files to Modify

- `src/services/geminiVisionService.ts`
- `src/components/StateCheckWizard.tsx`
- `src/contexts/DependencyContext.tsx`

### Implementation Steps

#### Step 2.1: Enhance Vision Service with Real Progress

**File:** `src/services/geminiVisionService.ts`

Add progress callback support to `analyzeStateFromImage`:

```typescript
export interface AnalysisProgressCallback {
  (stage: string, progress: number, estimatedTimeRemaining?: number): void;
}

export interface AnalysisOptions {
  timeout?: number;
  onProgress?: AnalysisProgressCallback;
  signal?: AbortSignal;
  useCache?: boolean;
}

export const analyzeStateFromImage = async (
  base64Image: string,
  options: AnalysisOptions = {}
): Promise<FacialAnalysis> => {
  const { 
    timeout = 45000,  // Increased from 30s
    onProgress, 
    signal, 
    useCache = true 
  } = options;
  
  // Stage 1: Cache check (5%)
  onProgress?.('Checking cache...', 5);
  
  const cacheKey = `vision:${base64Image.substring(0, 100)}`;
  if (useCache) {
    const cached = await cacheService.get<FacialAnalysis>(cacheKey);
    if (cached) {
      onProgress?.('Using cached results', 100);
      return cached;
    }
  }
  
  // Stage 2: Validate AI availability (10%)
  onProgress?.('Checking AI availability...', 10);
  
  const ai = getAI();
  if (!ai) {
    onProgress?.('AI not configured, using offline analysis', 100);
    return getOfflineAnalysis(base64Image);
  }
  
  // Stage 3: Prepare request (15%)
  onProgress?.('Preparing analysis request...', 15);
  
  // Stage 4: Send to AI (20-80%)
  onProgress?.('Analyzing facial features...', 20, Math.round(timeout / 1000));
  
  // ... rest of implementation with progress updates
};
```

#### Step 2.2: Add Offline Analysis Fallback

Enhance the offline fallback to provide meaningful estimates based on image properties:

```typescript
/**
 * Offline fallback analysis based on image metrics
 * Provides rough estimates when AI is unavailable
 */
const getOfflineAnalysis = async (base64Image: string): Promise<FacialAnalysis> => {
  // Analyze image brightness/contrast for lighting estimate
  const imageMetrics = await analyzeImageMetrics(base64Image);
  
  const observations: FacialAnalysis['observations'] = [];
  
  // Estimate lighting from image brightness
  if (imageMetrics.brightness < 0.3) {
    observations.push({
      category: 'lighting',
      value: 'low light conditions',
      evidence: 'detected from image brightness analysis',
    });
  } else if (imageMetrics.brightness > 0.7) {
    observations.push({
      category: 'lighting',
      value: 'bright lighting',
      evidence: 'detected from image brightness analysis',
    });
  }
  
  return {
    confidence: 0.3, // Low confidence for offline
    observations,
    lighting: imageMetrics.brightness < 0.3 ? 'low light' : 
              imageMetrics.brightness > 0.7 ? 'bright' : 'moderate',
    lightingSeverity: 'moderate',
    environmentalClues: ['Offline analysis - limited observations available'],
  };
};

async function analyzeImageMetrics(base64: string): Promise<{ brightness: number; contrast: number }> {
  return new Promise((resolve) => {
    const img = new Image();
    img.onload = () => {
      const canvas = document.createElement('canvas');
      const ctx = canvas.getContext('2d');
      if (!ctx) {
        resolve({ brightness: 0.5, contrast: 0.5 });
        return;
      }
      
      // Sample center region
      const sampleSize = 100;
      canvas.width = sampleSize;
      canvas.height = sampleSize;
      ctx.drawImage(img, 0, 0, sampleSize, sampleSize);
      
      const imageData = ctx.getImageData(0, 0, sampleSize, sampleSize);
      const data = imageData.data;
      
      let totalBrightness = 0;
      for (let i = 0; i < data.length; i += 4) {
        // Luminance formula
        totalBrightness += (data[i] * 0.299 + data[i+1] * 0.587 + data[i+2] * 0.114) / 255;
      }
      
      const avgBrightness = totalBrightness / (data.length / 4);
      resolve({ brightness: avgBrightness, contrast: 0.5 }); // Contrast TBD
    };
    img.onerror = () => resolve({ brightness: 0.5, contrast: 0.5 });
    img.src = base64.startsWith('data:') ? base64 : `data:image/png;base64,${base64}`;
  });
}
```

#### Step 2.3: Update StateCheckWizard Progress Display

**File:** `src/components/StateCheckWizard.tsx`

Connect real progress to UI:

```typescript
const handleCapture = async (src: string) => {
  setImageSrc(src);
  setStep('ANALYZING');
  setProgress(0);
  setError('');
  
  const base64 = src.split(',')[1];
  abortControllerRef.current = new AbortController();
  
  try {
    const result = await visionService.analyzeFromImage(base64, {
      timeout: 45000,
      signal: abortControllerRef.current.signal,
      onProgress: (stage, progressValue, timeRemaining) => {
        setCurrentStage(stage);
        setProgress(progressValue);
        if (timeRemaining !== undefined) {
          setEstimatedTime(timeRemaining);
        }
      },
    });
    
    setAnalysis(result);
    setStep('RESULTS');
    
  } catch (e) {
    // ... error handling
  }
};
```

### Testing Checklist for Phase 2

- [ ] Progress bar reflects real analysis stages
- [ ] Analysis completes successfully with valid results
- [ ] Timeout errors show helpful message
- [ ] Offline fallback provides reasonable estimates
- [ ] Cancel button aborts analysis properly
- [ ] Circuit breaker state prevents repeated failures

---

## Phase 3: Unified Data Flow

### Problem Analysis

Bio-Mirror results don't automatically flow into journal entries. The current flow:
- User opens Bio-Mirror â†’ Takes photo â†’ Sees results â†’ Results lost when closing
- Journal entry has `photoObservations` state but no way to populate it from Bio-Mirror

### Solution Architecture

Create a shared observation context that all capture components feed into:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ObservationProvider                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚   Visual    â”‚  â”‚    Audio    â”‚  â”‚    Text     â”‚        â”‚
â”‚  â”‚ (Bio-Mirror)â”‚  â”‚  (Voice)    â”‚  â”‚  (Journal)  â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚         â”‚                â”‚                â”‚                â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                          â–¼                                 â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚              â”‚ Combined Observationsâ”‚                      â”‚
â”‚              â”‚   + Correlations    â”‚                       â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                         â”‚                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚    JournalEntry     â”‚
              â”‚   (Consumes All)    â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Implementation Steps

#### Step 3.1: Create Observation Context

**File:** `src/contexts/ObservationContext.tsx`

```typescript
import React, { createContext, useContext, useReducer, useCallback, ReactNode } from 'react';
import { ObjectiveObservation, FacialAnalysis } from '../types';
import { AudioAnalysisResult } from '../services/audioAnalysisService';

// State shape
interface ObservationState {
  visual: ObjectiveObservation | null;
  audio: ObjectiveObservation | null;
  text: ObjectiveObservation | null;
  sessionId: string | null;
  lastUpdated: string | null;
}

// Actions
type ObservationAction =
  | { type: 'SET_VISUAL'; payload: FacialAnalysis }
  | { type: 'SET_AUDIO'; payload: AudioAnalysisResult }
  | { type: 'SET_TEXT'; payload: any }
  | { type: 'START_SESSION' }
  | { type: 'CLEAR_SESSION' }
  | { type: 'CLEAR_ALL' };

// Initial state
const initialState: ObservationState = {
  visual: null,
  audio: null,
  text: null,
  sessionId: null,
  lastUpdated: null,
};

// Reducer
function observationReducer(state: ObservationState, action: ObservationAction): ObservationState {
  const timestamp = new Date().toISOString();
  
  switch (action.type) {
    case 'SET_VISUAL':
      return {
        ...state,
        visual: {
          type: 'visual',
          source: 'bio-mirror',
          observations: action.payload.observations,
          confidence: action.payload.confidence,
          timestamp,
        },
        lastUpdated: timestamp,
      };
      
    case 'SET_AUDIO':
      return {
        ...state,
        audio: {
          type: 'audio',
          source: 'voice',
          observations: action.payload.observations,
          confidence: action.payload.confidence,
          timestamp,
        },
        lastUpdated: timestamp,
      };
      
    case 'SET_TEXT':
      return {
        ...state,
        text: {
          type: 'text',
          source: 'text-input',
          observations: action.payload.observations || [],
          confidence: action.payload.confidence || 0.8,
          timestamp,
        },
        lastUpdated: timestamp,
      };
      
    case 'START_SESSION':
      return {
        ...initialState,
        sessionId: `session_${Date.now()}`,
        lastUpdated: timestamp,
      };
      
    case 'CLEAR_SESSION':
      return initialState;
      
    case 'CLEAR_ALL':
      return initialState;
      
    default:
      return state;
  }
}

// Context
interface ObservationContextValue {
  state: ObservationState;
  setVisualObservation: (analysis: FacialAnalysis) => void;
  setAudioObservation: (analysis: AudioAnalysisResult) => void;
  setTextObservation: (data: any) => void;
  startSession: () => void;
  clearSession: () => void;
  getAllObservations: () => ObjectiveObservation[];
  hasObservations: () => boolean;
}

const ObservationContext = createContext<ObservationContextValue | null>(null);

// Provider
export function ObservationProvider({ children }: { children: ReactNode }) {
  const [state, dispatch] = useReducer(observationReducer, initialState);
  
  const setVisualObservation = useCallback((analysis: FacialAnalysis) => {
    dispatch({ type: 'SET_VISUAL', payload: analysis });
  }, []);
  
  const setAudioObservation = useCallback((analysis: AudioAnalysisResult) => {
    dispatch({ type: 'SET_AUDIO', payload: analysis });
  }, []);
  
  const setTextObservation = useCallback((data: any) => {
    dispatch({ type: 'SET_TEXT', payload: data });
  }, []);
  
  const startSession = useCallback(() => {
    dispatch({ type: 'START_SESSION' });
  }, []);
  
  const clearSession = useCallback(() => {
    dispatch({ type: 'CLEAR_SESSION' });
  }, []);
  
  const getAllObservations = useCallback((): ObjectiveObservation[] => {
    const observations: ObjectiveObservation[] = [];
    if (state.visual) observations.push(state.visual);
    if (state.audio) observations.push(state.audio);
    if (state.text) observations.push(state.text);
    return observations;
  }, [state.visual, state.audio, state.text]);
  
  const hasObservations = useCallback((): boolean => {
    return !!(state.visual || state.audio || state.text);
  }, [state.visual, state.audio, state.text]);
  
  return (
    <ObservationContext.Provider value={{
      state,
      setVisualObservation,
      setAudioObservation,
      setTextObservation,
      startSession,
      clearSession,
      getAllObservations,
      hasObservations,
    }}>
      {children}
    </ObservationContext.Provider>
  );
}

// Hook
export function useObservations() {
  const context = useContext(ObservationContext);
  if (!context) {
    throw new Error('useObservations must be used within ObservationProvider');
  }
  return context;
}
```

#### Step 3.2: Wrap App with ObservationProvider

**File:** `src/App.tsx`

Add the provider around the app content:

```typescript
import { ObservationProvider } from './contexts/ObservationContext';

// In the render:
<DependencyProvider dependencies={getDependencies()}>
  <ObservationProvider>
    <BrowserRouter>
      {/* ... rest of app */}
    </BrowserRouter>
  </ObservationProvider>
</DependencyProvider>
```

#### Step 3.3: Connect Bio-Mirror to Observation Context

**File:** `src/components/StateCheckWizard.tsx`

Add observation context integration:

```typescript
import { useObservations } from '../contexts/ObservationContext';

const StateCheckWizard: React.FC = () => {
  const { setVisualObservation } = useObservations();
  
  // ... existing code ...
  
  const handleCapture = async (src: string) => {
    // ... existing capture logic ...
    
    try {
      const result = await visionService.analyzeFromImage(base64);
      
      // Store in observation context for journal access
      setVisualObservation(result);
      
      setAnalysis(result);
      setStep('RESULTS');
    } catch (e) {
      // ... error handling
    }
  };
  
  // ... rest of component
};
```

#### Step 3.4: Update JournalEntry to Use Observation Context

**File:** `src/components/JournalEntry.tsx`

Replace local state with context:

```typescript
import { useObservations } from '../contexts/ObservationContext';

const JournalEntry: React.FC<Props> = ({ onEntryAdded }) => {
  const { 
    state: observationState, 
    getAllObservations, 
    clearSession,
    hasObservations,
  } = useObservations();
  
  // Remove local photoObservations state
  // const [photoObservations, setPhotoObservations] = useState<any>(null);
  
  // Use context observations directly
  const photoObservations = observationState.visual;
  const voiceObservations = observationState.audio;
  
  // Update handleSubmit to use context
  const handleSubmit = async () => {
    // ... existing logic ...
    
    // Get all observations from context
    const objectiveObservations = getAllObservations();
    
    const newEntry: HealthEntry = {
      // ... existing fields ...
      objectiveObservations: objectiveObservations.length > 0 ? objectiveObservations : undefined,
    };
    
    onEntryAdded(newEntry);
    clearSession(); // Clear observations after saving
    resetForm();
  };
  
  // Show indicator when observations exist
  {hasObservations() && (
    <div className="mb-4 p-3 bg-indigo-50 dark:bg-indigo-900/20 rounded-lg">
      <p className="text-sm text-indigo-700 dark:text-indigo-300">
        ðŸ“Š Objective observations captured and ready to include
      </p>
    </div>
  )}
};
```

### Testing Checklist for Phase 3

- [ ] Bio-Mirror results persist in observation context
- [ ] Voice analysis results persist in observation context
- [ ] Journal entry includes all captured observations
- [ ] Observations are cleared after saving entry
- [ ] UI shows indicator when observations exist
- [ ] Data flows correctly from capture to journal to storage

---

## Phase 4: Enhanced Self-Reported Data Logging

### Problem Analysis

Current journaling is basic text input. For neurodivergent users who may need to "ramble" or process verbally, enhancements needed:

1. Extended voice recording (5+ minutes)
2. Structured prompts based on capacity levels
3. Auto-save drafts
4. Timestamps within long entries

### Implementation Steps

#### Step 4.1: Add Extended Recording Mode

**File:** `src/components/RecordVoiceButton.tsx`

Add extended mode option:

```typescript
interface RecordVoiceButtonProps {
  onTranscript: (text: string, audioBlob?: Blob, analysis?: AudioAnalysisResult) => void;
  onAnalysisReady?: (analysis: AudioAnalysisResult) => void;
  isDisabled?: boolean;
  extendedMode?: boolean; // New prop
  maxDuration?: number;   // New prop - seconds
}

const RecordVoiceButton: React.FC<RecordVoiceButtonProps> = ({
  onTranscript,
  onAnalysisReady,
  isDisabled = false,
  extendedMode = false,
  maxDuration = extendedMode ? 600 : 300, // 10 min extended, 5 min normal
}) => {
  // Add timestamp tracking for extended recordings
  const [segments, setSegments] = useState<Array<{
    text: string;
    timestamp: number;
  }>>([]);
  
  // ... existing code with duration limit update
};
```

#### Step 4.2: Add Draft Auto-Save

**File:** `src/services/draftService.ts` (new file)

```typescript
const DRAFT_KEY = 'maeple_journal_draft';
const AUTOSAVE_INTERVAL = 30000; // 30 seconds

interface JournalDraft {
  text: string;
  capacity: CapacityProfile;
  observations: ObjectiveObservation[];
  lastSaved: string;
}

export const saveDraft = (draft: JournalDraft): void => {
  localStorage.setItem(DRAFT_KEY, JSON.stringify({
    ...draft,
    lastSaved: new Date().toISOString(),
  }));
};

export const loadDraft = (): JournalDraft | null => {
  const stored = localStorage.getItem(DRAFT_KEY);
  if (!stored) return null;
  
  try {
    const draft = JSON.parse(stored);
    // Only return draft if less than 24 hours old
    const savedTime = new Date(draft.lastSaved).getTime();
    const now = Date.now();
    if (now - savedTime > 24 * 60 * 60 * 1000) {
      clearDraft();
      return null;
    }
    return draft;
  } catch {
    return null;
  }
};

export const clearDraft = (): void => {
  localStorage.removeItem(DRAFT_KEY);
};
```

#### Step 4.3: Add Guided Prompts Component

**File:** `src/components/GuidedPrompts.tsx` (new file)

```typescript
import React from 'react';
import { CapacityProfile } from '../types';

interface Props {
  capacity: CapacityProfile;
  onSelectPrompt: (prompt: string) => void;
}

const PROMPTS_BY_CAPACITY: Record<string, Record<'low' | 'mid' | 'high', string[]>> = {
  focus: {
    low: [
      "What's pulling at your attention right now?",
      "What would help you feel more grounded?",
    ],
    mid: [
      "What task feels most achievable right now?",
    ],
    high: [
      "What do you want to focus your energy on?",
    ],
  },
  sensory: {
    low: [
      "What's overwhelming your senses right now?",
      "What would make your environment more comfortable?",
    ],
    mid: [
      "How is your environment affecting you?",
    ],
    high: [
      "Your sensory tolerance is high - is there anything you want to try?",
    ],
  },
  // ... more prompts for each dimension
};

export const GuidedPrompts: React.FC<Props> = ({ capacity, onSelectPrompt }) => {
  const getRelevantPrompts = (): string[] => {
    const prompts: string[] = [];
    
    Object.entries(capacity).forEach(([key, value]) => {
      if (typeof value !== 'number') return;
      
      const level = value <= 3 ? 'low' : value <= 6 ? 'mid' : 'high';
      const dimensionPrompts = PROMPTS_BY_CAPACITY[key]?.[level] || [];
      
      // Prioritize low capacity prompts
      if (value <= 3) {
        prompts.unshift(...dimensionPrompts);
      } else {
        prompts.push(...dimensionPrompts);
      }
    });
    
    return prompts.slice(0, 3); // Show max 3 prompts
  };
  
  const prompts = getRelevantPrompts();
  
  if (prompts.length === 0) return null;
  
  return (
    <div className="space-y-2 mb-4">
      <p className="text-xs text-slate-500 dark:text-slate-400">
        Optional prompts based on your capacity:
      </p>
      <div className="flex flex-wrap gap-2">
        {prompts.map((prompt, i) => (
          <button
            key={i}
            onClick={() => onSelectPrompt(prompt)}
            className="px-3 py-1.5 text-sm bg-slate-100 dark:bg-slate-700 rounded-full hover:bg-slate-200 dark:hover:bg-slate-600 transition-colors"
          >
            {prompt}
          </button>
        ))}
      </div>
    </div>
  );
};
```

### Testing Checklist for Phase 4

- [ ] Extended recording works for 5+ minutes
- [ ] Drafts auto-save every 30 seconds
- [ ] Drafts are restored on page reload
- [ ] Guided prompts appear based on capacity levels
- [ ] Selecting a prompt adds it to the text area

---

## Phase 5: Analytics & Correlation Enhancement

### Problem Analysis

The analytics service has correlation logic but doesn't leverage the new multi-source observations effectively.

### Implementation Steps

#### Step 5.1: Add Real-Time Correlation Service

**File:** `src/services/correlationService.ts` (new file)

```typescript
import { HealthEntry, ObjectiveObservation, CapacityProfile } from '../types';

export interface Correlation {
  type: 'match' | 'mismatch' | 'trend';
  source1: string;
  source2: string;
  description: string;
  confidence: number;
  significance: 'low' | 'medium' | 'high';
}

export interface CorrelationResult {
  correlations: Correlation[];
  dissociationRisk: number; // 0-1, high means subjective â‰  objective
  recommendations: string[];
}

export function analyzeCorrelations(
  capacity: CapacityProfile,
  observations: ObjectiveObservation[],
  recentEntries: HealthEntry[] = []
): CorrelationResult {
  const correlations: Correlation[] = [];
  let dissociationRisk = 0;
  const recommendations: string[] = [];
  
  const visualObs = observations.find(o => o.type === 'visual');
  const audioObs = observations.find(o => o.type === 'audio');
  
  // Compare subjective capacity with objective observations
  if (visualObs) {
    const fatigueObs = visualObs.observations.find(o => o.category === 'fatigue');
    
    // High self-reported physical + objective fatigue signs
    if (fatigueObs && capacity.physical >= 7) {
      correlations.push({
        type: 'mismatch',
        source1: 'Self-reported physical energy',
        source2: 'Visual fatigue indicators',
        description: 'You report high physical energy, but visual analysis detected fatigue signs.',
        confidence: visualObs.confidence,
        significance: 'high',
      });
      dissociationRisk += 0.3;
      recommendations.push('Consider taking a break - your body may be more tired than you realize.');
    }
    
    // Tension correlation
    const tensionObs = visualObs.observations.find(o => o.category === 'tension');
    if (tensionObs && capacity.emotional >= 7) {
      correlations.push({
        type: 'mismatch',
        source1: 'Self-reported emotional capacity',
        source2: 'Visual tension indicators',
        description: 'You report high emotional capacity, but tension was detected.',
        confidence: visualObs.confidence,
        significance: 'medium',
      });
      dissociationRisk += 0.2;
    }
  }
  
  if (audioObs) {
    const noiseObs = audioObs.observations.find(o => o.category === 'noise');
    
    // High sensory tolerance + high noise environment
    if (noiseObs?.severity === 'high' && capacity.sensory >= 7) {
      correlations.push({
        type: 'mismatch',
        source1: 'Self-reported sensory tolerance',
        source2: 'Environmental noise level',
        description: 'High noise detected. Your sensory system may be working harder than you realize.',
        confidence: audioObs.confidence,
        significance: 'medium',
      });
      recommendations.push('Consider moving to a quieter environment or using noise-canceling headphones.');
    }
  }
  
  // Check for matches (validation)
  if (visualObs && !visualObs.observations.find(o => o.category === 'fatigue') && capacity.physical <= 4) {
    correlations.push({
      type: 'match',
      source1: 'Self-reported low energy',
      source2: 'Visual observations',
      description: 'Your self-assessment aligns with objective observations.',
      confidence: visualObs.confidence,
      significance: 'low',
    });
  }
  
  // Normalize dissociation risk
  dissociationRisk = Math.min(dissociationRisk, 1);
  
  return {
    correlations,
    dissociationRisk,
    recommendations,
  };
}
```

#### Step 5.2: Add Correlation Display Component

**File:** `src/components/CorrelationInsights.tsx` (new file)

```typescript
import React from 'react';
import { AlertTriangle, CheckCircle, TrendingUp } from 'lucide-react';
import { Correlation, CorrelationResult } from '../services/correlationService';

interface Props {
  result: CorrelationResult;
  onDismiss?: () => void;
}

export const CorrelationInsights: React.FC<Props> = ({ result, onDismiss }) => {
  const { correlations, dissociationRisk, recommendations } = result;
  
  if (correlations.length === 0) return null;
  
  const getIcon = (type: Correlation['type']) => {
    switch (type) {
      case 'mismatch': return <AlertTriangle size={16} className="text-amber-500" />;
      case 'match': return <CheckCircle size={16} className="text-green-500" />;
      case 'trend': return <TrendingUp size={16} className="text-blue-500" />;
    }
  };
  
  return (
    <div className="bg-slate-50 dark:bg-slate-800/50 rounded-xl p-4 space-y-3">
      <div className="flex items-center justify-between">
        <h4 className="font-semibold text-slate-800 dark:text-slate-200">
          ðŸ“Š Pattern Insights
        </h4>
        {onDismiss && (
          <button 
            onClick={onDismiss}
            className="text-slate-400 hover:text-slate-600"
          >
            Ã—
          </button>
        )}
      </div>
      
      {dissociationRisk > 0.5 && (
        <div className="bg-amber-50 dark:bg-amber-900/20 border border-amber-200 dark:border-amber-800 rounded-lg p-3">
          <p className="text-sm text-amber-800 dark:text-amber-200">
            <strong>Notice:</strong> There's a difference between how you feel and what we observed. 
            This could indicate masking or early burnout signs.
          </p>
        </div>
      )}
      
      <div className="space-y-2">
        {correlations.map((corr, i) => (
          <div key={i} className="flex items-start gap-2 text-sm">
            {getIcon(corr.type)}
            <p className="text-slate-700 dark:text-slate-300">{corr.description}</p>
          </div>
        ))}
      </div>
      
      {recommendations.length > 0 && (
        <div className="border-t border-slate-200 dark:border-slate-700 pt-3 mt-3">
          <p className="text-xs font-medium text-slate-500 dark:text-slate-400 mb-2">
            Suggestions:
          </p>
          <ul className="space-y-1">
            {recommendations.map((rec, i) => (
              <li key={i} className="text-sm text-slate-600 dark:text-slate-300">
                â€¢ {rec}
              </li>
            ))}
          </ul>
        </div>
      )}
    </div>
  );
};
```

### Testing Checklist for Phase 5

- [ ] Correlations are calculated when observations exist
- [ ] Mismatches are detected (high self-report + objective signs)
- [ ] Matches are validated (alignment confirmation)
- [ ] Dissociation risk is calculated correctly
- [ ] Recommendations are contextually relevant
- [ ] Insights display in journal view

---

## Implementation Order & Dependencies

```
Phase 1 (Camera Fix)
    â”‚
    â”œâ”€â”€â–º Can be done independently
    â”‚
    â–¼
Phase 2 (Analysis Chain)
    â”‚
    â”œâ”€â”€â–º Depends on Phase 1 for stable capture
    â”‚
    â–¼
Phase 3 (Data Flow)
    â”‚
    â”œâ”€â”€â–º Depends on Phases 1 & 2 for data sources
    â”‚
    â–¼
Phase 4 (Logging)          Phase 5 (Analytics)
    â”‚                           â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â”œâ”€â”€â–º Can be done in parallel
                â”‚
                â–¼
           Integration Testing
```

---

## File Change Summary

### New Files to Create
1. `src/hooks/useCameraCapture.ts`
2. `src/contexts/ObservationContext.tsx`
3. `src/services/draftService.ts`
4. `src/services/correlationService.ts`
5. `src/components/GuidedPrompts.tsx`
6. `src/components/CorrelationInsights.tsx`

### Files to Modify
1. `src/components/BiofeedbackCameraModal.tsx` - Major refactor
2. `src/components/StateCheckCamera.tsx` - Major refactor
3. `src/components/StateCheckWizard.tsx` - Context integration
4. `src/components/JournalEntry.tsx` - Context integration
5. `src/components/RecordVoiceButton.tsx` - Extended mode
6. `src/services/geminiVisionService.ts` - Progress callbacks
7. `src/App.tsx` - Add ObservationProvider
8. `src/hooks/index.ts` - Export new hook

### Files to Test
1. All camera-related components
2. All journal flow components
3. Analytics dashboard
4. Bio-Mirror results display

---

## Success Metrics

After implementation, verify:

1. **Camera Stability**: Zero flickering during normal use
2. **Analysis Completion Rate**: >95% of analyses complete successfully
3. **Data Persistence**: 100% of observations flow to journal entries
4. **User Experience**: Smooth flow from capture â†’ analysis â†’ journal
5. **Performance**: No memory leaks, fast page transitions

---

## Notes for Implementation

1. **Test each phase independently** before moving to the next
2. **Keep existing components working** during refactor (incremental changes)
3. **Add console logging** during development for debugging
4. **Run TypeScript compiler** after each change to catch type errors
5. **Test on mobile devices** - camera behavior differs significantly
6. **Check React DevTools** for unnecessary re-renders

---

_Plan created: January 4, 2026_
_Target completion: January 7, 2026_
